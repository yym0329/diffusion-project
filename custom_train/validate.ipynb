{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/miniconda3/envs/train/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/home/work/miniconda3/envs/train/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import contextlib\n",
    "import gc\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "\n",
    "from neuraltexture_controlnet import NeuralTextureControlNetModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "def import_model_class_from_model_name_or_path(\n",
    "    pretrained_model_name_or_path: str, revision: str\n",
    "):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "        revision=revision,\n",
    "    )\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "\n",
    "        return CLIPTextModel\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")\n",
    "\n",
    "\n",
    "\n",
    "text_encoder_cls = import_model_class_from_model_name_or_path(\n",
    "\t\"stabilityai/stable-diffusion-2-1-base\", None\n",
    ")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-2-1-base\",\n",
    "        subfolder=\"unet\",   \n",
    "    )\n",
    "weight_dtype = torch.float32\n",
    "# Load scheduler and models\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "\t\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"scheduler\"\n",
    ")\n",
    "text_encoder = text_encoder_cls.from_pretrained(\n",
    "\t\"stabilityai/stable-diffusion-2-1-base\",\n",
    "\tsubfolder=\"text_encoder\",\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "\t\"stabilityai/stable-diffusion-2-1-base\",\n",
    "\tsubfolder=\"vae\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\t\"stabilityai/stable-diffusion-2-1-base\",\n",
    "\tsubfolder=\"tokenizer\",\n",
    "\tuse_fast=False,\n",
    ")\n",
    "controlnet_dir = '/data2/diffusion-project/diffusion-project/custom_train/dilightnet-openillum-2-1-2-base-v2'\n",
    "controlnet = NeuralTextureControlNetModel.from_pretrained(controlnet_dir, torch_dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7fb12f10f049ebbc534a2d250a28cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-2-1-base\",\n",
    "        tokenizer=tokenizer,\n",
    "        unet=unet,\n",
    "        controlnet=controlnet,\n",
    "        safety_checker=None,\n",
    "        torch_dtype=weight_dtype)\n",
    "pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline = pipeline.to('cuda')\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 128it [00:00, 177654.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from relighting_dataset import RelightingDataset\n",
    "validation_dataset = RelightingDataset(\n",
    "\tdata_jsonl=\"/data2/diffusion-project/diffusion-project/custom_train/dataset_v2/eval_v2.jsonl\",\n",
    "\tpretrained_model= \"stabilityai/stable-diffusion-2-1-base\",\n",
    "\tchannel_aug_ratio= 0,  # add to args\n",
    "\tempty_prompt_ratio= 0,  # add to args\n",
    "\tlog_encode_hint=False,  # add to args\n",
    "\tload_mask=True,  # add to args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(None, None, None) TypeError('list indices must be integers or slices, not str') list indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "batch = validation_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(None, None, None) TypeError('list indices must be integers or slices, not str') list indices must be integers or slices, not str\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "prompt = batch[\"text\"]\n",
    "validation_image = batch[\"conditioning_pixel_values\"].to(\n",
    "\t'cuda', dtype=weight_dtype\n",
    ")[None]\n",
    "\n",
    "images = []\n",
    "for _ in range(4):\n",
    "\twith torch.autocast(\"cuda\"):\n",
    "\t\timage = pipeline(\n",
    "\t\t\tprompt,\n",
    "\t\t\tvalidation_image,\n",
    "\t\t\tnum_inference_steps=100,\n",
    "\t\t\t\n",
    "\t\t).images[0]\n",
    "\timages.append(image)\n",
    "\n",
    "cond_pixels = batch[\n",
    "\t\"conditioning_pixel_values\"\n",
    "]  # hints. [mask, ref image , diffuse, 3*ggx]\n",
    "\n",
    "if args.add_mask:\n",
    "\tcond_pixels = cond_pixels[1:]  # skip mask\n",
    "# to numpy\n",
    "cond_pixels = cond_pixels.cpu().numpy()\n",
    "target_image = batch[\"pixel_values\"].cpu().numpy()\n",
    "target_image = (target_image / 2.0 + 0.5).transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipeline(prompt, validation_image, num_inference_steps=30).images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
